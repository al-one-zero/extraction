{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "722AglgvY91w"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#importer le dataframe de train prétraité par preprocessing.py\n",
        "df = pd.read_pickle(\"../data/df_preproc.pk\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amAt_5iD_a5W"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mylTk1jF9aI",
        "outputId": "c42fa7d4-103c-45c8-cac2-ff9ebef79e6d"
      },
      "source": [
        "!pip install emot\n",
        "!pip install fasttext\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq3kTh3sZhyt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "import tokenization\n",
        "\n",
        "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiAwY6zhf6lS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#codage des labels\n",
        "catmap = {\"pos\" : 0, \"neg\" : 1, \"neu\" : 2, \"irr\" : 3}\n",
        "#décodage\n",
        "invmap = {0 : \"pos\", 1 : \"neg\", 2 : \"neu\", 3 : \"irr\"}\n",
        "\n",
        "X = [\"Tweet\", \"Language\"]\n",
        "y = \"Avis\"\n",
        "\n",
        "df_train_test = df[X + [y]].copy()\n",
        "df_train_test[y] = df_train_test[y].apply(lambda x : catmap[x])\n",
        "\n",
        "#partition en données de train/test\n",
        "train, test = train_test_split(df_train_test, test_size=0.2, random_state=1000)\n",
        "\n",
        "#exclusion de la classe \"irr\"\n",
        "train = train[train.Avis != catmap[\"irr\"]]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdLZWsMlafXh"
      },
      "source": [
        "#vectorisation des tweets\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZFBi5xcap-j"
      },
      "source": [
        "#construction du modèle\n",
        "def build_model(bert_layer, max_len=512, lr=1e-5):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    net = tf.keras.layers.Dense(10, activation='relu')(clf_output)\n",
        "    net = tf.keras.layers.Dropout(0.2)(net)\n",
        "    net = tf.keras.layers.Dense(10, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(0.2)(net)\n",
        "    out = tf.keras.layers.Dense(3, activation='softmax')(net)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tSSaLGIaxzG"
      },
      "source": [
        "max_len = 150\n",
        "train_input = bert_encode(train.Tweet.values, tokenizer, max_len=max_len)\n",
        "#test_input = bert_encode(test.Tweet.values, tokenizer, max_len=max_len)\n",
        "train_labels = tf.keras.utils.to_categorical(train.Avis.values, num_classes=3)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAbReEkza1_2",
        "outputId": "5640296a-9c92-4879-9174-d95b5a8d81af"
      },
      "source": [
        "#entrainement du modèle\n",
        "model = build_model(bert_layer, max_len=max_len, lr=0.00001)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_history = model.fit(\n",
        "    train_input, train_labels, \n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=32,\n",
        "    verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "56/56 [==============================] - 41s 629ms/step - loss: 1.0930 - accuracy: 0.4196 - val_loss: 0.6698 - val_accuracy: 0.7658\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.76577, saving model to model.h5\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - 35s 619ms/step - loss: 0.7937 - accuracy: 0.6305 - val_loss: 0.6241 - val_accuracy: 0.7725\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.76577 to 0.77252, saving model to model.h5\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - 35s 618ms/step - loss: 0.7100 - accuracy: 0.6865 - val_loss: 0.5730 - val_accuracy: 0.7995\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.77252 to 0.79955, saving model to model.h5\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - 35s 618ms/step - loss: 0.6957 - accuracy: 0.6775 - val_loss: 0.6050 - val_accuracy: 0.8131\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.79955 to 0.81306, saving model to model.h5\n",
            "Epoch 5/10\n",
            "56/56 [==============================] - 35s 618ms/step - loss: 0.6799 - accuracy: 0.6937 - val_loss: 0.5627 - val_accuracy: 0.8198\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.81306 to 0.81982, saving model to model.h5\n",
            "Epoch 6/10\n",
            "56/56 [==============================] - 35s 618ms/step - loss: 0.6514 - accuracy: 0.6984 - val_loss: 0.6472 - val_accuracy: 0.8131\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.81982\n",
            "Epoch 7/10\n",
            "56/56 [==============================] - 35s 619ms/step - loss: 0.5996 - accuracy: 0.7163 - val_loss: 0.6673 - val_accuracy: 0.8086\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.81982\n",
            "Epoch 8/10\n",
            "56/56 [==============================] - 35s 618ms/step - loss: 0.5715 - accuracy: 0.7324 - val_loss: 0.6143 - val_accuracy: 0.8221\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.81982 to 0.82207, saving model to model.h5\n",
            "Epoch 9/10\n",
            "56/56 [==============================] - 35s 619ms/step - loss: 0.5451 - accuracy: 0.7519 - val_loss: 0.7413 - val_accuracy: 0.8041\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.82207\n",
            "Epoch 10/10\n",
            "56/56 [==============================] - 35s 618ms/step - loss: 0.5614 - accuracy: 0.7523 - val_loss: 0.6353 - val_accuracy: 0.8176\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.82207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_RHX5Y-a7qE",
        "outputId": "8dba74d9-7e1e-4f85-d37b-93f0aa0c7c5b"
      },
      "source": [
        "#évaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "test_input = bert_encode(test.Tweet.values, tokenizer, max_len=max_len)\n",
        "test_output = test.Avis.values\n",
        "pred = np.argmax(model.predict(test_input), axis=1)\n",
        "\n",
        "#prédiction de la classe \"irr\" à partir de la langue du tweet\n",
        "for i in range(len(test)):\n",
        "  if test.Language[i] != \"en\":\n",
        "    pred[i] = 3\n",
        "\n",
        "#calcul du score\n",
        "accuracy_score(test_output, pred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8143712574850299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frrl9DBrGvPp"
      },
      "source": [
        "#prédiction des labels de test.txt\n",
        "#importation des données test.txt prétraitées par preprocessing.py\n",
        "df_test = pd.read_pickle(\"../data/test_df.pk\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhDR-mulGgFT"
      },
      "source": [
        "#vectorisation\n",
        "test_input = bert_encode(df_test.Tweet.values, tokenizer, max_len=max_len)\n",
        "#prédiction des classes \"pos\", \"neg\" et \"neu\"\n",
        "pred = np.argmax(model.predict(test_input), axis=1)\n",
        "\n",
        "#prédiction de la classe \"irr\"\n",
        "for i in range(len(df_test)):\n",
        "  if df_test.Language[i] != \"en\":\n",
        "    pred[i] = 3\n",
        "\n",
        "#décodage des classes\n",
        "pred = [invmap[p] for p in pred]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bOmi_zPG8Zl"
      },
      "source": [
        "df_test[\"pred\"] = pred"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwEjvVGeHbtL",
        "outputId": "9ee40c09-b701-4e4a-cc4b-52a033c89bdb"
      },
      "source": [
        "#stats\n",
        "df_test.pred.value_counts()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neu    499\n",
              "irr    288\n",
              "neg    116\n",
              "pos     97\n",
              "Name: pred, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHfiUwvVHvRc"
      },
      "source": [
        "#exportation du résultat\n",
        "def to_txt(in_df, raw_df):\n",
        "    res = \"\"\n",
        "    for i in range(len(in_df)):\n",
        "        res += \"({0},{1},{2}) {3}\".format(in_df.index[i], in_df.pred[i], in_df.Entreprise[i], raw_df.Tweet[i])\n",
        "    return res\n",
        "\n",
        "save_to = \"../data/test_output.txt\"\n",
        "\n",
        "with open(save_to, \"w\") as text_file:\n",
        "    text_file.write(to_txt(df_test, df_test))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teEbqFcUIa6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}