{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import pandas as pd\\nfrom functools import reduce\\n\\ndata = pd.read_pickle(\\\"../data/df_preproc.pk\\\")\\nhashtags_vocab = reduce((lambda x, y: set(x).union(y)), data.Hashtags)\\nmentions_vocab = reduce((lambda x, y: set(x).union(y)), data.Mentions)\";\n",
       "                var nbb_formatted_code = \"import pandas as pd\\nfrom functools import reduce\\n\\ndata = pd.read_pickle(\\\"../data/df_preproc.pk\\\")\\nhashtags_vocab = reduce((lambda x, y: set(x).union(y)), data.Hashtags)\\nmentions_vocab = reduce((lambda x, y: set(x).union(y)), data.Mentions)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "data = pd.read_pickle(\"../data/df_preproc.pk\")\n",
    "hashtags_vocab = reduce((lambda x, y: set(x).union(y)), data.Hashtags)\n",
    "mentions_vocab = reduce((lambda x, y: set(x).union(y)), data.Mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"import tensorflow as tf\\n\\nimport tensorflow_hub as hub\\n\\nfrom tensorflow.keras.layers import Embedding, Flatten\\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\\n\\n\\ndef embedding_model(vocab, max_len, max_features, out_dim):\\n    vectorize_layer = TextVectorization(\\n        max_tokens=max_features,\\n        output_sequence_length=max_len,\\n        output_mode=\\\"int\\\",\\n        # split=None,\\n        vocabulary=list(vocab),\\n    )\\n    return tf.keras.Sequential(\\n        layers=[vectorize_layer, Embedding(max_features + 1, out_dim), Flatten()]\\n    )\\n\\n\\nmax_hashtags = 5\\nmax_mentions = 5\\n\\n_embed = hub.load(\\\"https://tfhub.dev/google/nnlm-en-dim128/2\\\")\\n_hashtag_embed = embedding_model(hashtags_vocab, 8, 2000, 16)\\n_mention_embed = embedding_model(mentions_vocab, 8, 1500, 16)\";\n",
       "                var nbb_formatted_code = \"import tensorflow as tf\\n\\nimport tensorflow_hub as hub\\n\\nfrom tensorflow.keras.layers import Embedding, Flatten\\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\\n\\n\\ndef embedding_model(vocab, max_len, max_features, out_dim):\\n    vectorize_layer = TextVectorization(\\n        max_tokens=max_features,\\n        output_sequence_length=max_len,\\n        output_mode=\\\"int\\\",\\n        # split=None,\\n        vocabulary=list(vocab),\\n    )\\n    return tf.keras.Sequential(\\n        layers=[vectorize_layer, Embedding(max_features + 1, out_dim), Flatten()]\\n    )\\n\\n\\nmax_hashtags = 5\\nmax_mentions = 5\\n\\n_embed = hub.load(\\\"https://tfhub.dev/google/nnlm-en-dim128/2\\\")\\n_hashtag_embed = embedding_model(hashtags_vocab, 8, 2000, 16)\\n_mention_embed = embedding_model(mentions_vocab, 8, 1500, 16)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "\n",
    "def embedding_model(vocab, max_len, max_features, out_dim):\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=max_features,\n",
    "        output_sequence_length=max_len,\n",
    "        output_mode=\"int\",\n",
    "        # split=None,\n",
    "        vocabulary=list(vocab),\n",
    "    )\n",
    "    return tf.keras.Sequential(\n",
    "        layers=[vectorize_layer, Embedding(max_features + 1, out_dim), Flatten()]\n",
    "    )\n",
    "\n",
    "\n",
    "max_hashtags = 5\n",
    "max_mentions = 5\n",
    "\n",
    "_embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "_hashtag_embed = embedding_model(hashtags_vocab, 8, 2000, 16)\n",
    "_mention_embed = embedding_model(mentions_vocab, 8, 1500, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128)\n",
      "(2, 128)\n",
      "(5, 128)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"print(_hashtag_embed.predict([[\\\"google apple\\\"], [\\\"app quote\\\"]]).shape)\\nprint(_mention_embed.predict([[\\\"google apple\\\"], [\\\"tesla\\\"]]).shape)\\nprint(_embed(data.Tweet[10:15]).shape)\";\n",
       "                var nbb_formatted_code = \"print(_hashtag_embed.predict([[\\\"google apple\\\"], [\\\"app quote\\\"]]).shape)\\nprint(_mention_embed.predict([[\\\"google apple\\\"], [\\\"tesla\\\"]]).shape)\\nprint(_embed(data.Tweet[10:15]).shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(_hashtag_embed.predict([[\"google apple\"], [\"app quote\"]]).shape)\n",
    "print(_mention_embed.predict([[\"google apple\"], [\"tesla\"]]).shape)\n",
    "print(_embed(data.Tweet[10:15]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x1573e51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x1573e51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x1573dfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x1573dfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"from thinc.api import (\\n    Model,\\n    chain,\\n    strings2arrays,\\n    with_array,\\n    HashEmbed,\\n    expand_window,\\n    Relu,\\n    Softmax,\\n    Adam,\\n    warmup_linear,\\n    TensorFlowWrapper,\\n)\\nfrom thinc.types import List1d, Array2d\\n\\nembed_layer = hub.KerasLayer(\\n    \\\"https://tfhub.dev/google/nnlm-en-dim128/2\\\", input_shape=[], dtype=tf.string\\n)\\nembed = tf.keras.Sequential()\\nembed.add(embed_layer)\\nembed: Model[List1d, Array2d] = TensorFlowWrapper(embed)\";\n",
       "                var nbb_formatted_code = \"from thinc.api import (\\n    Model,\\n    chain,\\n    strings2arrays,\\n    with_array,\\n    HashEmbed,\\n    expand_window,\\n    Relu,\\n    Softmax,\\n    Adam,\\n    warmup_linear,\\n    TensorFlowWrapper,\\n)\\nfrom thinc.types import List1d, Array2d\\n\\nembed_layer = hub.KerasLayer(\\n    \\\"https://tfhub.dev/google/nnlm-en-dim128/2\\\", input_shape=[], dtype=tf.string\\n)\\nembed = tf.keras.Sequential()\\nembed.add(embed_layer)\\nembed: Model[List1d, Array2d] = TensorFlowWrapper(embed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from thinc.api import (\n",
    "    Model,\n",
    "    chain,\n",
    "    strings2arrays,\n",
    "    with_array,\n",
    "    HashEmbed,\n",
    "    expand_window,\n",
    "    Relu,\n",
    "    Softmax,\n",
    "    Adam,\n",
    "    warmup_linear,\n",
    "    TensorFlowWrapper,\n",
    ")\n",
    "from thinc.types import List1d, Array2d\n",
    "\n",
    "embed_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/nnlm-en-dim128/2\", input_shape=[], dtype=tf.string\n",
    ")\n",
    "embed = tf.keras.Sequential()\n",
    "embed.add(embed_layer)\n",
    "embed: Model[List1d, Array2d] = TensorFlowWrapper(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout = 0.2\n",
    "\n",
    "with Model.define_operators({\">>\": chain}):\n",
    "    model = (\n",
    "        embed\n",
    "        >> Relu(nO=n_hidden, dropout=dropout)\n",
    "        >> Relu(nO=n_hidden, dropout=dropout)\n",
    "        >> Softmax()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
